[{"content":"It has been a while studying and working with the Chromecast Protocol. This protocol is proprietary, and officially only the Chrome browser, Android, and iOS platforms support casting to a Chromecast-enabled device.\nNow, Chromium is open source, and so is Android, but we would not want anything to do with Android for obvious reasons, not to mention this feature may be tucked away in Google Play Services.\nSo we settle with Chromium which is written in C++. Thankfully almost all of the hard work was done by many other open source projects, notably Node CastV2. Not only this project provides a simple implementation for the Chromecast protocol but also a written explanation of the underlying stuff and how things actually work.\nAll I need to figure out is how to implement that is C.\nA brief overview of the connection and communication is as follows:\nWe start by discovering the Chromecast devices using mDNS (Avahi) for devices named \u0026ldquo;googlecast\u0026rdquo; Next up we open a socket connection to the selected device on port 8009 We then negotiate a TLS connection on the same socket connection Now comes the part where we open a \u0026ldquo;virtual connection\u0026rdquo; with the device With that done, the Chromecast device expects a \u0026ldquo;PING\u0026rdquo; message every 5 seconds to keep the virtual connection alive We are in the position to send and receive messages now The messages are JSON encoded and serialized in protobuf according to the proto file conveniently found in the Chromium source code. Before sending the binary over, we prefix it with 4 bytes of packet length in Big Endian order. JSON is universal, good and easy to understand mostly because it is text, but what about protobuf? It is binary. Handy-dandy library protobuf-c save the day.\nHere is what the proto file looks like:\n// Copyright 2013 The Chromium Authors. All rights reserved. // Use of this source code is governed by a BSD-style license that can be // found in the LICENSE file. syntax = \u0026#34;proto2\u0026#34;; option optimize_for = LITE_RUNTIME; package castchannel; message CastMessage { enum ProtocolVersion { CASTV2_1_0 = 0; } required ProtocolVersion protocol_version = 1; required string source_id = 2; required string destination_id = 3; required string namespace = 4; enum PayloadType { STRING = 0; BINARY = 1; } required PayloadType payload_type = 5; optional string payload_utf8 = 6; optional bytes payload_binary = 7; } message AuthChallenge { } message AuthResponse { required bytes signature = 1; required bytes client_auth_certificate = 2; } message AuthError { enum ErrorType { INTERNAL_ERROR = 0; NO_TLS = 1; } required ErrorType error_type = 1; } message DeviceAuthMessage { optional AuthChallenge challenge = 1; optional AuthResponse response = 2; optional AuthError error = 3; } We can send many types of messages like CONNECT, PING, GET_STATUS, GET_APP_AVAILIBILITY, STOP, CLOSE, etc.\nThese messages are sent in their respective namespaces (some of them are: urn:x-cast:com.google.cast.tp.connection, urn:x-cast:com.google.cast.tp.heartbeat, urn:x-cast:com.google.cast.receiver) along with some other values such as sender_id, destination_id, payload_type, payload_utf8, etc.\nDifferent namespaces don\u0026rsquo;t mean we create new channels for each one. They are just like the other values in the protobuf message.\nWe won\u0026rsquo;t bother ourselves to verify the authenticity of the Chromecast device we are talking to. Thus all the Auth- messages shall not be entertained. We are much more interested in the CastMessage.\nProtobuf-c comes with a lot of helper functions, but it offers yet more. We use protoc, \u0026ldquo;a code generator that converts Protocol Buffer .proto files to C descriptor code\u0026rdquo;.\nHaving the TLS connection set up, we serialize a connect message and send it over.\nFor some odd reason when I sent { \u0026quot;type\u0026quot;: \u0026quot;CONNECT\u0026quot; }, it replied with { \u0026quot;type\u0026quot;: \u0026quot;CLOSE\u0026quot; }, replied { \u0026quot;type\u0026quot;: \u0026quot;PING\u0026quot; } with { \u0026quot;type\u0026quot;: \u0026quot;PONG\u0026quot; }, and { \u0026quot;type\u0026quot;: \u0026quot;GET_STATUS\u0026quot; } with { \u0026quot;type\u0026quot;: \u0026quot;CLOSE\u0026quot; }.\nWhen I insisted on connection multiple times, it got mad and stopped replying to my messages :/\nIt still played ping pong though (replied to all the ping messages with pong).\nWe haven\u0026rsquo;t come to terms since then. Was it because I left it hanging, and I have a feeling it must have waited for my messages all night?\nI tried to sever all ties and make them all over again, shut it down and revived it, changed my address (local IP), and changed my name (sender_id), but who knows what is going inside that little mind of it.\nFear not our next mission involves peering into that mind and getting to the root cause of this confusion (Custom Web Receiver).\nWe didn\u0026rsquo;t talk about the Custom Web Receiver this time and the protocols that would be implemented. Hang tight because that is what we are going to do in the coming days.\nMany thanks to Benjamin and Claudio! I couldn\u0026rsquo;t have made it without their help and advice.\nRelevant Links:\nMerge Request Chromecast Docs Node CastV2 Chromium Source Code Protobuf-c Cast Framework Messages ","date":"2022-07-05T00:00:00Z","image":"https://kyteinsky.github.io/p/chromecast-connect/google-cast-logo_hu380fea45a8713440d3703426d8db7476_197587_120x120_fill_q75_box_smart1.jpg","permalink":"https://kyteinsky.github.io/p/chromecast-connect/","title":"Chromecast Connect"},{"content":"I will be working as a Google Summer of Code Intern at GNOME Foundation, and my project will be to add Chromecast support to the already very cool GNOME Network Displays app that has Miracast support as of now. It can be installed through flatpak as well.\nDetails Linux desktop users will be able to cast their screens to Miracast and Chromecast devices. For the Android TVs that feature both of them, it would be wise to opt for the Chromecast path when both the devices share the same router (i.e. are on the same network) since Miracast would require setting up a WiFi-Direct connection to function (may be faster with less latency, may not be worth the trouble).\nI believe this will see the light of day with very helpful and experienced mentors, Claudio Wunder and Benjamin Berg.\nProgress We started by looking into the Chromecast documentation (an adventure in itself). So there are two ends to deal with - the sender one and the receiver one.\nThe receiver end There are two options here: Styled Media Receiver and Custom Web Receiver.\nThe Styled Media Receiver does everything for us. It hosts the receiver application (an HTML5 app) and provides all the default styles and functionality. If we want to change the styles and/or the logo, we are free to host them on an HTTPS server, and we\u0026rsquo;ll be done.\nWith everything it offers, it has limited still quite adequate media support, including playing video files, audio files, streaming video, and audio, displaying images and opening specific apps on the TV (like YouTube).\nOur main job is to \u0026ldquo;stream\u0026rdquo; our desktop to the Chromecast device. For this purpose, the Styled Media Receiver supports three streaming protocols:\nDynamic Adaptive Streaming over HTTP (DASH) Apple\u0026rsquo;s HTTP Live Streaming (HLS) Microsoft\u0026rsquo;s Smooth streaming We drop considerations for the Smooth Streaming protocol here because of the lack of feature differences compared to the other widely supported protocols, except that HLS and Smooth Streaming support pre-loading by default.\nSo we end up here with two choices: DASH and HLS. Now, HLS is widely supported; on the other hand, DASH is codec agnostic for both video and audio. In terms of latency, both are said to have similar latencies that depend on the segment duration we decide upon.\nAs per the HLS\u0026rsquo;s RFC (linked below), the EXT-X-TARGETDURATION tag in the playlist file (or manifest file) accepts a decimal integer value. It dictates how much the maximum duration of each segment can be (rounded off to the nearest integer). It is suspected to be similar to DASH, although its RFC does not clearly mention so. We better find out by actually experimenting and streaming.\nReducing the segment size too much can cause the bitrate to bump up and the stream quality to degrade thanks to all the added keyframes for each segment, so we need to test what suits this live stream the best.\nNext up, we take the discussion to the Custom Web Receiver and the conclusions from some of our tests with the Chromecast streamings using the Command and Control (CaC) Tool, which is a sender app provided by Google for testing and debugging Web Receiver apps.\nWe want to test out and know more about other protocols not supported out of the box in Chromecast but can work fine with a Custom Web Receiver: SRT, RTSP and WebRTC.\nThanks to Benjamin and Claudio for all the help!\nRelevant Links:\nChromecast Docs Styled Media Receiver Streaming Protocols Supported by Web Receiver DASH vs HLS by Toolbox Common Video Streaming Protocols Blog on Segment Length in DASH and HLS Blog on Segment Duration in DASH and HLS HLS RFC: EXT-X-TARGETDURATION tag DASH RFC CaC Tool ","date":"2022-06-12T00:00:00Z","image":"https://kyteinsky.github.io/p/gnome-introductory-post/GNOME-logo-500x316_huc3a6bb4b474f9f0abf2bbc70e8fa6968_11449_120x120_fill_box_smart1_3.png","permalink":"https://kyteinsky.github.io/p/gnome-introductory-post/","title":"GNOME Introductory Post"},{"content":"Â The Minikube Cluster It is a local playground to learn or test your code as it would run on a full-fledged Kubernetes Cluster. It creates a small cluster consisting of only one node.\nThe command minikube start launches the cluster with options for bare-metal, ssh, docker among others. The cluster can be managed with CLI but it also offers a GUI in the web browser, can be launched with minikube dashboard command.\nThe task of minikube is over here. Now we head on to using the kubectl command to manage the cluster.\nManage the Cluster Information gathering can be done easily using the command kubectl get followed by pods, services, deployments, replicaset, etc. A deployment can be easily created (using docker here) by running\n$ kubectl create deployment \u0026lt;name\u0026gt; --image=\u0026lt;docker-image-name\u0026gt; deployment.apps/\u0026lt;name\u0026gt; created Now we can view the pods created using command:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE \u0026lt;name\u0026gt;-\u0026lt;random-string\u0026gt; 1/1 Running 0 5s There is only one pod and only one replicaset (kubectl get replicasets), because that is the default value, can be changed with --replicas flag, just like\n$ kubectl create deployment \u0026lt;name\u0026gt; --image=\u0026lt;docker-image-name\u0026gt; --replicas=3 Doing things this way would be slightly cumbersome when there were many different pods with different configurations, moreover they are hard to track, hence I will be using an easier way to deploy the cluster - using YAML configuration files.\nThe following config from this YouTube video creates a nginx cluster with 3 replicas and exposes the port 8080\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-depl labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.16 ports: - containerPort: 8080 And it can be deployed using kubectl apply -f filename.yml\nAbove was an example where three replicas were created with one pod each with one container each, we can also create one pod with two containers which have totally different docker images. This YAML configuration from this helpful website below does exactly that.\napiVersion: v1 kind: Pod metadata: name: nginx-pod spec: volumes: - name: html emptyDir: {} containers: - name: first image: nginx volumeMounts: - name: html mountPath: /usr/share/nginx/html - name: second image: debian volumeMounts: - name: html mountPath: /html command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;] args: - while true; do date \u0026gt;\u0026gt; /html/index.html; sleep 1; done It is a pod configuration that attaches a common volume to both the containers named html, which is an empty directory. Then it defines the configuration for the two containers named first and second with docker images nginx and docker respectively. In the first container, the html volume is mounted at /usr/share/nginx/html so that nginx would serve content from there, and the second container is just a linux destribution that mounts the html volume at location /html in its file structure and executes the command /bin/sh -c with these arguments\nwhile true; do date \u0026gt;\u0026gt; /html/index.html; sleep 1; done This piece of code just appends the /html/index.html file and sets it content to be the present datetime.\nLong story short debian writes data to the index.html file which is inside the html volume, every second, and nginx reads from the same volume\nOne can create the pod using the YAML file with the following command:\n$ kubectl create -f nginx-pod.yml pod/nginx-pod created The pod can be viewed using\n$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-pod 2/2 Running 0 6s Now the index.html file can be viewed by running cat command inside the container\n$ kubectl exec nginx-pod -c first -- /bin/cat /usr/share/nginx/html/index.html # for the first container Thu Jul 7 10:41:36 UTC 2021 Thu Jul 7 10:41:37 UTC 2021 Thu Jul 7 10:41:38 UTC 2021 Thu Jul 7 10:41:39 UTC 2021 $ kubectl exec nginx-pod -c second -- /bin/cat /html/index.html # for the second container Thu Jul 7 10:41:36 UTC 2021 Thu Jul 7 10:41:37 UTC 2021 Thu Jul 7 10:41:38 UTC 2021 Thu Jul 7 10:41:39 UTC 2021 Thu Jul 7 10:41:40 UTC 2021 Thu Jul 7 10:41:41 UTC 2021 Thu Jul 7 10:41:42 UTC 2021 Thu Jul 7 10:41:43 UTC 2021 Thu Jul 7 10:41:44 UTC 2021 Thu Jul 7 10:41:45 UTC 2021 Thu Jul 7 10:41:46 UTC 2021 Thu Jul 7 10:41:47 UTC 2021 Thu Jul 7 10:41:48 UTC 2021 kubectl describe po command can be used to get detailed information about the pods running in the cluster.\n","date":"2021-07-06T00:00:00Z","image":"https://kyteinsky.github.io/p/playing-around-with-kubernetes-in-ec2/kubernetes-logo_hu2ea33729aed5bb392e760ba17b57be76_83730_120x120_fill_box_smart1_3.png","permalink":"https://kyteinsky.github.io/p/playing-around-with-kubernetes-in-ec2/","title":"Playing Around with Kubernetes in EC2"},{"content":"Â Inside the AWS Console: Explored compute machine options\nEC2 is the easiest of all, provides vanilla virtual machine experience for fully self-managing the infrastructure Lambda is for small fuctions or API calls App Runner provides a full-managed infrastructure to run our app handling all the scaling, load balancing, installation of software and deployment. Elastic Beanstalk is a similar service to App Runner, but there is a slight difference, Elastic Beanstalk provides more control over infrastructure after deployment, whereas App Runner manages it fully even after deployment. (source) Explored storage options\nS3, Simple Storage Service, used for every purpose like storing files and also hosting static websites EFS, Elastic File System, used to store files that can be attached to multiple EC2 instances, are fully-managed and scale automatically FSx is used for Windows Servers and playing nice with its ecosystem VPC\nUsed to create a private space in cloud for related, non-public services to live in One or more services can be exposed to the internet and other VPCs or AWS services, while secure services like databases can be secured and only accessed by some hidden service inside the VPC making it more secure Creating two EC2 instances inside a VPC and one S3 bucket using AWS CLI and then clearing off all the resources (VPC deleted through Console because of security group being of type \u0026lsquo;default\u0026rsquo; was not being deleted through CLI for some convoluted reason ","date":"2021-06-29T00:00:00Z","image":"https://kyteinsky.github.io/p/aws-console-and-cli-exploration/aws-logo-full_hu521aa76c031b70f3e5f473d7c126ac09_325263_120x120_fill_box_smart1_3.png","permalink":"https://kyteinsky.github.io/p/aws-console-and-cli-exploration/","title":"AWS Console and CLI Exploration"},{"content":"Â Ensure the prerequisites An AWS Account AWS cli installed, instructions for the same can be found here. Have generated credentials (access key and secret), can be created from here. Setting up AWS CLI Type aws configure on the terminal and enter the Access Key ID and Secret Access Key, other fields can be blank.\nCreating a config file for terraform to understand Create a directory and switch to it Create a file named main.tf and populate it with the following sample configuration,\nwhere app_server is the name of the instance us-west-2 is the region where the instance will be deployed ami-830c94e3 is the unique identifier for the instance t2.micro is the instance type ExampleAppServerInstance is the tag name for grouping of instances terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 3.27\u0026#34; } } required_version = \u0026#34;\u0026gt;= 0.14.9\u0026#34; } provider \u0026#34;aws\u0026#34; { profile = \u0026#34;default\u0026#34; region = \u0026#34;us-west-2\u0026#34; } resource \u0026#34;aws_instance\u0026#34; \u0026#34;app_server\u0026#34; { ami = \u0026#34;ami-830c94e3\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; tags = { Name = \u0026#34;ExampleAppServerInstance\u0026#34; } } Installation of and Working with Terraform CLI: Terraform can be installed following this guide Configuration of terraform CLI can be done using command terraform init The configuration file can be formatted using terraform fmt command And the file can be validated using terraform validate command Finally all the changes can be viewed using terraform plan command and perfomed using terraform apply command Here is a terminal cast performing the above actions ","date":"2021-06-27T00:00:00Z","image":"https://kyteinsky.github.io/p/installing-terraform-and-creating-an-ec2-instance/terraform-logo-extended_hu9aa061a1aaf8d97b03487600ef22b7fc_104790_120x120_fill_q75_box_smart1.jpg","permalink":"https://kyteinsky.github.io/p/installing-terraform-and-creating-an-ec2-instance/","title":"Installing Terraform and Creating an EC2 Instance"},{"content":"Â Azure part of story First create one Azure account, I am using a student account Create a basic instance (Ubuntu image works for most cases) with 1 vCPU and 1 GiB RAM, or according to your needs Download the private key and SSH into the instance Update the packages Install docker (following this guide) Let\u0026rsquo;s move on to the local machine setup\nLocal machine setup SSH Keys Setup We need to install docker here as well Add ssh key (downloaded from Azure portal) to OpenSSH Auth agent eval \u0026#39;ssh-agent\u0026#39; ssh-add \u0026lt;path-to-pem-file\u0026gt;/\u0026lt;file-name\u0026gt;.pem (or) Add AddKeysToAgent yes to ~/.ssh/config file to automatically do that for you echo \u0026#34;AddKeysToAgent yes\u0026#34; \u0026gt;\u0026gt; ~/.ssh/config Docker Context Setup Create a new docker context to talk to the docker in Azure instance with command docker context create azure-box --docker \u0026#34;host=ssh://azureuser@\u0026lt;public-ip-of-instance\u0026gt;\u0026#34; See all available contexts using docker context ls Switching to Azure docker is done using command docker context use azure-box and back to the local docker using docker context use default Now when we issue commands to docker as usual docker ps, it is directly communicating to the docker running in the Azure instance Note: \u0026lsquo;azure-box\u0026rsquo; is just a name, can be replaced with anything of your choice\nðŸŽ‰ Now you have docker running in the cloud, accessible from the terminal Finally just for testing, we can spin up a Ubuntu container and bash into it: ","date":"2021-06-15T00:00:00Z","image":"https://kyteinsky.github.io/p/hosting-docker-in-cloud-azure/rafael-garcin-sqZ4GeyYGx8-unsplash_hu124ff0668c422682b8055a7bbd405272_397209_120x120_fill_q75_box_smart1.jpg","permalink":"https://kyteinsky.github.io/p/hosting-docker-in-cloud-azure/","title":"Hosting Docker in Cloud (Azure)"},{"content":"How to add and commit changes in the and then push the changes to a remote repository (here GitHub) Outline of steps to be performed: Change some files or create some Issuegit add \u0026lt;name of files\u0026gt;command to add files to be committed Then commit them using git commit -am \u0026lt;commit message\u0026gt;, here a is for all files, m is for message, and then you place your message for the changes (optional) Finally, push the changes to a remote repository Let\u0026rsquo;s see this in action ","date":"2021-06-11T00:00:00Z","image":"https://kyteinsky.github.io/p/git-add-git-commit-and-git-push/roman-synkevych-UT8LMo-wlyk-unsplash_hue2e581fbbca1ab8a23c99d56e2dd85e8_1034377_120x120_fill_q75_box_smart1.jpg","permalink":"https://kyteinsky.github.io/p/git-add-git-commit-and-git-push/","title":"Git Add, Git Commit and Git Push"},{"content":"Â Following a general approach for any theme, steps to be performed (following this article): Specific for the theme Stack: Click Here\nGet code for asciinema player wget https://github.com/asciinema/asciinema-player/releases/download/v2.6.1/asciinema-player.css wget https://github.com/asciinema/asciinema-player/releases/download/v2.6.1/asciinema-player.js Place them in static/css and static/js, where static is folder in the root dir of the website Copy the layouts/partials/_shared/head.html or layouts/partials/head/head.html from themes/\u0026lt;theme-name\u0026gt; directory to your website\u0026rsquo;s root directory with the same folder structure Do the same with layouts/_default/baseof.html Append this code block for css in layouts/partials/{head or _shared}/head.html {{ if .Params.asciinema }} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;{{ .Site.BaseURL }}css/asciinema-player.css\u0026#34; /\u0026gt; {{ end }} And this for js in layouts/_default/baseof.html {{ if .Params.asciinema }} \u0026lt;script defer src=\u0026#34;{{ .Site.BaseURL }}js/asciinema-player.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{ end }} Now let\u0026rsquo;s make a shortcode for asciinema here: layouts/shortcodes/asciinema.html One can change the default rows and columns attributes inside \u0026lt;asciinema-player\u0026gt; tag\n\u0026lt;p\u0026gt; \u0026lt;asciinema-player src=\u0026#34;/casts/{{ with .Get \u0026#34;key\u0026#34; }}{{ . }}{{ end }}\u0026#34; cols=\u0026#34;{{ if .Get \u0026#34;cols\u0026#34; }}{{ .Get \u0026#34;cols\u0026#34; }}{{ else }}640{{ end }}\u0026#34; rows=\u0026#34;{{ if .Get \u0026#34;rows\u0026#34; }}{{ .Get \u0026#34;rows\u0026#34; }}{{ else }}10{{ end }}\u0026#34; {{ if .Get \u0026#34;autoplay\u0026#34; }}autoplay=\u0026#34;{{ .Get \u0026#34;autoplay\u0026#34; }}\u0026#34;{{ end }} {{ if .Get \u0026#34;preload\u0026#34; }}preload=\u0026#34;{{ .Get \u0026#34;preload\u0026#34; }}\u0026#34;{{ end }} {{ if .Get \u0026#34;loop\u0026#34; }}loop=\u0026#34;{{ .Get \u0026#34;loop\u0026#34; }}\u0026#34;{{ end }} start-at=\u0026#34;{{ if .Get \u0026#34;start-at\u0026#34; }}{{ .Get \u0026#34;start-at\u0026#34; }}{{ else }}0{{ end }}\u0026#34; speed=\u0026#34;{{ if .Get \u0026#34;speed\u0026#34; }}{{ .Get \u0026#34;speed\u0026#34; }}{{ else }}1{{ end }}\u0026#34; {{ if .Get \u0026#34;idle-time-limit\u0026#34; }}idle-time-limit=\u0026#34;{{ .Get \u0026#34;idle-time-limit\u0026#34; }}\u0026#34;{{ end }} {{ if .Get \u0026#34;poster\u0026#34; }}poster=\u0026#34;{{ .Get \u0026#34;poster\u0026#34; }}\u0026#34;{{ end }} {{ if .Get \u0026#34;font-size\u0026#34; }}font-size=\u0026#34;{{ .Get \u0026#34;font-size\u0026#34; }}\u0026#34;{{ end }} {{ if .Get \u0026#34;theme\u0026#34; }}theme=\u0026#34;{{ .Get \u0026#34;theme\u0026#34; }}\u0026#34;{{ end }} {{ if .Get \u0026#34;title\u0026#34; }}title=\u0026#34;{{ .Get \u0026#34;title\u0026#34; }}\u0026#34;{{ end }} {{ if .Get \u0026#34;author\u0026#34; }}author=\u0026#34;{{ .Get \u0026#34;author\u0026#34; }}\u0026#34;{{ end }} {{ if .Get \u0026#34;author-url\u0026#34; }}author-url=\u0026#34;{{ .Get \u0026#34;author-url\u0026#34; }}\u0026#34;{{ end }} {{ if .Get \u0026#34;author-img-url\u0026#34; }}author-img-url=\u0026#34;{{ .Get \u0026#34;author-img-url\u0026#34; }}\u0026#34;{{ end }} \u0026gt;\u0026lt;/asciinema-player\u0026gt; \u0026lt;/p\u0026gt; Finally create a new post and set asciinema = true in the metadata (for toml) And embed the asciinema cast as follows in the body part {{\u0026lt; asciinema key=\u0026#34;demo-folder/demo-cast.cast\u0026#34; rows=\u0026#34;35\u0026#34; preload=\u0026#34;1\u0026#34; \u0026gt;}} And don\u0026rsquo;t forget to put your .cast file in static/casts/demo-folder/democast.cast First attempt at the task Correcting the mistakes done above For the Stack theme, the kind developer of the theme @zhixuan666 mentioned a more efficient way to do it. Here is his reply to the GitHub issue: link\n","date":"2021-06-10T00:00:00Z","permalink":"https://kyteinsky.github.io/p/embedding-asciinema-in-hugo-websites/","title":"Embedding Asciinema in Hugo Websites"},{"content":"We create a new Hugo website in terminal and install a custom theme named Stack and create a sample page Outline of steps to be performed: Create an empty hugo site with boilerplate code Initialize the site as a git repository Add the theme as a submodule to receive updates automatically, can be just cloned in place or downloaded and extracted at the right place manually Copy some example site\u0026rsquo;s posts and config file to the root folder of the website to edit them without disturbing the original theme files (these files override the respective default ones) Edit the config.yaml file as per requirements, and delete the config.toml file that was placed by hugo Create a new post with sone markdown text host the site locally This might be the right moment to commit the changes as starting point of the repository. Here is the asciinema terminal cast: ","date":"2021-06-01T00:00:00Z","image":"https://kyteinsky.github.io/p/new-hugo-website/hugo-logo-wide.svg","permalink":"https://kyteinsky.github.io/p/new-hugo-website/","title":"New Hugo Website"}]